{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vehicle verify --specification vehicle/properties.vcl --verifier Marabou --network classifier:adv_mid_bs32_0.999_e8_adv.onnx --dataset trainingInputs:vehicle/trainingInputs.idx --property property > vehicle/outputs/adv_mid_bs32_0.999_e8_adv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "\n",
    "def get_valid_inputs(data, n, save=False):\n",
    "    valid_inputs = []\n",
    "\n",
    "    for point in data:\n",
    "        valid_input = True\n",
    "        valid_tcp = False\n",
    "        valid_http = False\n",
    "        valid_time_elapsed = False\n",
    "\n",
    "        for element in point:\n",
    "            if element < 0.0 or element > 1.0:\n",
    "                valid_input = False\n",
    "                break\n",
    "            \n",
    "        # Flag, Size, Direction, Protocol\n",
    "        if point[7] == 2/256 and point[8] == 18/256 and point[9] == 16/256 and \\\n",
    "           point[17] >= 52/1000 and point[17] <= 60/1000 and point[18] >= 52/1000 and point[18] <= 60/1000 and point[19] >= 40/1000 and point[19] <= 52/1000 and \\\n",
    "           point[2] == 0 and point[3] == 1 and point[4] == 0 and \\\n",
    "           point[1] == 0:\n",
    "            valid_tcp = True\n",
    "\n",
    "        # Flag, Size, Direction, Protocol\n",
    "        if point[10] == 24/256 and \\\n",
    "           point[20] >= 0/1000 and point[20] <= 500/1000 and \\\n",
    "           point[5] == 0:\n",
    "            valid_http = True\n",
    "\n",
    "        # TimeElapsed\n",
    "        if point[0] == 0 or point[0] >= 0.1:\n",
    "            valid_time_elapsed = True\n",
    "\n",
    "        if valid_input and valid_tcp and valid_http and valid_time_elapsed:\n",
    "            valid_inputs.append(point)\n",
    "        \n",
    "        if len(valid_inputs) >= n:\n",
    "            break\n",
    "\n",
    "        if save:\n",
    "            idx2numpy.convert_to_file('./vehicle/trainingInputs.idx', valid_inputs)\n",
    "      \n",
    "    return valid_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "counter-example found for input 139\n",
      "counter-example found for input 176\n",
      "counter-example found for input 177\n",
      "counter-example found for input 325\n",
      "counter-example found for input 526\n",
      "counter-example found for input 528\n",
      "counter-example found for input 621\n",
      "counter-example found for input 698\n",
      "counter-example found for input 795\n",
      "counter-example found for input 865\n",
      "counter-example found for input 878\n",
      "counter-example found for input 952\n",
      "counter-example found for input 957\n",
      "counter-example found for input 966\n",
      "[139, 176, 177, 325, 526, 528, 621, 698, 795, 865, 878, 952, 957, 966]\n",
      "Adv acc: 0.9860\n",
      "Adv attack uccess rate: 0.0140\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "\n",
    "\n",
    "pgd_steps = 5\n",
    "batch_size = 32\n",
    "\n",
    "train_data_df = pd.read_csv(f'./data/full-preprocessed-ssh-data-train.csv')\n",
    "train_pos = np.array(train_data_df.loc[train_data_df.iloc[:, -1] == 0].iloc[:, :(len(train_data_df.columns)-1)].values.tolist())\n",
    "valid_inputs = np.array(get_valid_inputs(train_pos, n=1000))\n",
    "\n",
    "hyperrectangles = []\n",
    "hyperrectangles_labels = []\n",
    "\n",
    "# Define \"safe region\" hyperrectangles around input points\n",
    "for data in valid_inputs:\n",
    "    hyperrectangle = []\n",
    "    for i, d in enumerate(data):\n",
    "        if i == 14:\n",
    "            hyperrectangle.append([1e-7, 5e-6])\n",
    "        elif i == 19:\n",
    "            hyperrectangle.append([40/1000, 52/1000])\n",
    "        else:\n",
    "            hyperrectangle.append([d, d])\n",
    "    hyperrectangles.append(hyperrectangle)\n",
    "    hyperrectangles_labels.append(0)\n",
    "\n",
    "hyperrectangles = np.array(hyperrectangles)\n",
    "hyperrectangles_labels = np.array(hyperrectangles_labels)\n",
    "\n",
    "# Load model (need tensorflow==2.15)\n",
    "model_name = 'adv_mid_bs32_0.999_e8_adv'\n",
    "model = keras.models.load_model(model_name)\n",
    "\n",
    "\n",
    "# Initialise PGD\n",
    "pgd_attack_single_image_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "adv_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "pgd_dataset = []\n",
    "pgd_labels = []\n",
    "\n",
    "# Generate an adversarial input by PGD for each safe input, whithin their hyperrectangle \n",
    "for i, hyperrectangle in enumerate(hyperrectangles):\n",
    "    t_hyperrectangle = np.transpose(hyperrectangle)\n",
    "\n",
    "    # Calculate the epsilon for PGD for each dimension as ((dim[1] - dim[0]) / (pgd_steps * eps_multiplier))\n",
    "    eps = []\n",
    "    for d in hyperrectangle:\n",
    "        eps.append((d[1] - d[0]) / pgd_steps)\n",
    "    \n",
    "    # Generate an starting point for PGD by randomly sampling in the hyperrectangle \n",
    "    pgd_point = []\n",
    "    for d in hyperrectangle:\n",
    "        pgd_point.append(np.random.uniform(d[0], d[1]))\n",
    "    \n",
    "    pgd_point = tf.convert_to_tensor([pgd_point], dtype=tf.float32)\n",
    "    pgd_label = tf.convert_to_tensor([hyperrectangles_labels[i]], dtype=tf.float32)\n",
    "\n",
    "    # PGD attack\n",
    "    for _ in range(pgd_steps):\n",
    "        # Compute PGD loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(pgd_point)\n",
    "            prediction = model(pgd_point, training=False)\n",
    "            pgd_single_image_loss = pgd_attack_single_image_loss(pgd_label, prediction)\n",
    "        # Get the gradients of the loss w.r.t to the input point.\n",
    "        gradient = tape.gradient(pgd_single_image_loss, pgd_point)\n",
    "        # Get the sign of the gradient to create the perturbation\n",
    "        signed_grad = tf.sign(gradient)\n",
    "        # Apply perturbation to image\n",
    "        pgd_point = pgd_point + signed_grad * eps\n",
    "        pgd_point = tf.clip_by_value(pgd_point, t_hyperrectangle[0], t_hyperrectangle[1])\n",
    "        # print(f\"PGD step: {pgd_step + 1}\", end=\"\\r\")\n",
    "\n",
    "    # Concatenate the adversarial inputs and the original labels\n",
    "    if len(pgd_dataset) > 0:\n",
    "        pgd_dataset = np.concatenate((pgd_dataset, pgd_point), axis=0)\n",
    "        pgd_labels = np.concatenate((pgd_labels, pgd_label), axis=0)\n",
    "    else:\n",
    "        pgd_dataset = pgd_point\n",
    "        pgd_labels = pgd_label\n",
    "\n",
    "pgd_dataset = np.asarray(pgd_dataset)\n",
    "pgd_labels = np.asarray(pgd_labels)\n",
    "\n",
    "# Not all adversarial attacks are successful, so we check each adversarial inputs.\n",
    "counterexamples_dataset = []\n",
    "ce_idx = []\n",
    "\n",
    "for i, point in enumerate(pgd_dataset):\n",
    "    prediction = model.predict(tf.convert_to_tensor([point]), verbose=0)\n",
    "    class_index = np.argmax(prediction)\n",
    "    \n",
    "    if class_index != pgd_labels[i]:\n",
    "        print(f\"counter-example found for input {i}\")\n",
    "        ce_idx.append(i)\n",
    "        counterexamples_dataset.append(point)\n",
    "    \n",
    "print(ce_idx)\n",
    "# Print model's accuracy against adversarial attacks and adversarial attacks success rate\n",
    "adv_acc_2 = 1 - (len(counterexamples_dataset) / len(pgd_dataset))\n",
    "success_rate = (len(counterexamples_dataset) / len(pgd_dataset))\n",
    "\n",
    "print(f'Adv acc: {adv_acc_2:.4f}')\n",
    "print(f'Adv attack uccess rate: {success_rate:.4f}')\n",
    "\n",
    "# Save the counterexamples to a CSV file\n",
    "path = 'counterexamples/pgd'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "np.savetxt(f'{path}/{model_name}.csv', counterexamples_dataset, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the regex pattern to match the desired data\n",
    "pattern = r'x: \\[(.*?)\\]'\n",
    "\n",
    "models_directory = './models/tf/SSH'\n",
    "outputs_directory = './vehicle/outputs'\n",
    "for output_file in sorted(os.listdir(outputs_directory)):\n",
    "    output_path = os.path.join(outputs_directory, output_file)\n",
    "    cube_name = '.'.join(output_file.split('.')[:-1])\n",
    "\n",
    "    model = keras.models.load_model(os.path.join(models_directory, cube_name))\n",
    "\n",
    "\n",
    "    # Read the content of the file\n",
    "    with open(output_path, 'r') as file:\n",
    "        \n",
    "        text = file.read()\n",
    "\n",
    "        # Find all matches\n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        # Process each match and convert to a list of lists of floats\n",
    "        data = []\n",
    "        for match in matches:\n",
    "            # Convert the string of numbers into a list of floats\n",
    "            numbers = [float(num.strip()) for num in match.split(',')]\n",
    "            data.append(numbers)\n",
    "\n",
    "        # Convert the list of lists to a NumPy array\n",
    "        data_array = np.array(data)\n",
    "\n",
    "        adv_acc = 1 - (len(data_array) / 1000)\n",
    "        # success_rate = (len(data_array) / 1000)\n",
    "\n",
    "        print(f'Adv acc: {adv_acc:.4f} for {cube_name}')\n",
    "\n",
    "        counterexamples_dataset = []\n",
    "\n",
    "        for i, point in enumerate(data_array):\n",
    "            prediction = model.predict(tf.convert_to_tensor([point]), verbose=0)\n",
    "            class_index = np.argmax(prediction)\n",
    "            \n",
    "            if class_index != 0:\n",
    "                counterexamples_dataset.append(point)\n",
    "\n",
    "        adv_acc_2 = 1 - (len(counterexamples_dataset) / 1000)\n",
    "        success_rate = (len(counterexamples_dataset) / 1000)\n",
    "\n",
    "        print(f'Adv acc: {adv_acc_2:.4f} for {cube_name}')\n",
    "        print(f'Success rate: {success_rate:.4f} for {cube_name}')\n",
    "\n",
    "        # Save the NumPy array to a CSV file\n",
    "        path = 'counterexamples/vehicle'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        np.savetxt(f'{path}/{cube_name}.csv', counterexamples_dataset, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial",
   "language": "python",
   "name": "adversarial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
